# üìä Grafana Integration Guide / –†—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å Grafana

## üéØ –¶–µ–ª–∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏

–°–æ–∑–¥–∞–Ω–∏–µ –∫—Ä–∞—Å–∏–≤—ã—Ö –∏ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –¥–∞—à–±–æ—Ä–¥–æ–≤ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–æ–≤ Glacier –≤ Grafana:
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ YAML –æ—Ç—á–µ—Ç–æ–≤
- Real-time –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Å–µ—Ç–µ–≤—ã—Ö —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
- –ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –∏ —Ç—Ä–µ–Ω–¥—ã
- –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã –∏ –¥—Ä–∏–ª–¥–∞—É–Ω—ã
- –ê–ª–µ—Ä—Ç—ã –Ω–∞ –∞–Ω–æ–º–∞–ª—å–Ω—É—é –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å

## üèóÔ∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã

### –í–∞—Ä–∏–∞–Ω—Ç 1: PostgreSQL + TimescaleDB (–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
```
–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä ‚Üí YAML ‚Üí Parser ‚Üí PostgreSQL ‚Üí Grafana
```

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- –ü–æ–ª–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ SQL
- –û—Ç–ª–∏—á–Ω–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
- –†–∞–∑–≤–∏—Ç–∞—è —ç–∫–æ—Å–∏—Å—Ç–µ–º–∞
- –ü—Ä–æ—Å—Ç–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Grafana

### –í–∞—Ä–∏–∞–Ω—Ç 2: InfluxDB (–î–ª—è –º–µ—Ç—Ä–∏–∫ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏)
```
–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä ‚Üí YAML ‚Üí Parser ‚Üí InfluxDB ‚Üí Grafana
```

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
- –í—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ —Ç–µ–≥–æ–≤
- –í—ã—Å–æ–∫–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
- Retention policies

### –í–∞—Ä–∏–∞–Ω—Ç 3: JSON API (–ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç)
```
–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä ‚Üí YAML ‚Üí JSON API ‚Üí Grafana
```

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- –ë—ã—Å—Ç—Ä–∞—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞
- –ì–∏–±–∫–æ—Å—Ç—å
- –ú–∏–Ω–∏–º—É–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π

## üöÄ –†–µ–∞–ª–∏–∑–∞—Ü–∏—è (PostgreSQL –≤–∞—Ä–∏–∞–Ω—Ç)

### Docker Compose –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

```yaml
# docker-compose.grafana.yml
version: '3.8'

services:
  postgres:
    image: timescale/timescaledb:latest-pg14
    container_name: analyzer-postgres
    environment:
      POSTGRES_DB: analyzer_metrics
      POSTGRES_USER: analyzer_user
      POSTGRES_PASSWORD: analyzer_password
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./sql/init.sql:/docker-entrypoint-initdb.d/init.sql
    networks:
      - analyzer-network

  grafana:
    image: grafana/grafana:latest
    container_name: analyzer-grafana
    environment:
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: analyzer_admin
      GF_INSTALL_PLUGINS: grafana-clock-panel,grafana-simple-json-datasource,grafana-worldmap-panel
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning
      - ./grafana/dashboards:/var/lib/grafana/dashboards
    depends_on:
      - postgres
    networks:
      - analyzer-network

  yaml-processor:
    build: ./yaml-processor
    container_name: analyzer-yaml-processor
    environment:
      POSTGRES_HOST: postgres
      POSTGRES_DB: analyzer_metrics
      POSTGRES_USER: analyzer_user
      POSTGRES_PASSWORD: analyzer_password
      WATCH_DIRECTORY: /app/reports
    volumes:
      - ./reports:/app/reports:ro
      - ./yaml-processor:/app
    depends_on:
      - postgres
    networks:
      - analyzer-network

volumes:
  postgres_data:
  grafana_data:

networks:
  analyzer-network:
    driver: bridge
```

### –°—Ö–µ–º–∞ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö

```sql
-- sql/init.sql
-- –í–∫–ª—é—á–∞–µ–º TimescaleDB
CREATE EXTENSION IF NOT EXISTS timescaledb;

-- –û—Å–Ω–æ–≤–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
CREATE TABLE connections (
    time TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    hostname TEXT NOT NULL,
    source_address INET,
    destination_address INET,
    source_port INTEGER,
    destination_port INTEGER,
    protocol TEXT,
    protocol_number INTEGER,
    packet_count BIGINT DEFAULT 0,
    byte_count BIGINT DEFAULT 0,
    duration_ms INTEGER DEFAULT 0,
    tcp_flags INTEGER,
    direction TEXT, -- incoming/outgoing
    process_name TEXT,
    connection_state TEXT,
    report_id TEXT,
    
    -- –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    os_name TEXT,
    os_version TEXT,
    analyzer_version TEXT
);

-- –î–µ–ª–∞–µ–º —Ç–∞–±–ª–∏—Ü—É –≥–∏–ø–µ—Ä—Ç–∞–±–ª–∏—Ü–µ–π TimescaleDB
SELECT create_hypertable('connections', 'time');

-- –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø—Ä–æ—Ç–æ–∫–æ–ª–∞–º
CREATE TABLE protocol_stats (
    time TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    hostname TEXT NOT NULL,
    protocol TEXT NOT NULL,
    connection_count INTEGER DEFAULT 0,
    total_bytes BIGINT DEFAULT 0,
    total_packets BIGINT DEFAULT 0,
    unique_destinations INTEGER DEFAULT 0,
    report_id TEXT
);

SELECT create_hypertable('protocol_stats', 'time');

-- –¢–æ–ø –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–π
CREATE TABLE top_destinations (
    time TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    hostname TEXT NOT NULL,
    destination_address INET,
    destination_port INTEGER,
    connection_count INTEGER DEFAULT 0,
    total_bytes BIGINT DEFAULT 0,
    protocols TEXT[],
    processes TEXT[],
    report_id TEXT
);

SELECT create_hypertable('top_destinations', 'time');

-- –°–∏—Å—Ç–µ–º–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
CREATE TABLE system_metrics (
    time TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    hostname TEXT NOT NULL,
    total_connections INTEGER DEFAULT 0,
    incoming_connections INTEGER DEFAULT 0,
    outgoing_connections INTEGER DEFAULT 0,
    tcp_connections INTEGER DEFAULT 0,
    udp_connections INTEGER DEFAULT 0,
    icmp_connections INTEGER DEFAULT 0,
    unique_processes INTEGER DEFAULT 0,
    unique_destinations INTEGER DEFAULT 0,
    report_generation_time_ms INTEGER DEFAULT 0,
    os_name TEXT,
    os_version TEXT,
    report_id TEXT
);

SELECT create_hypertable('system_metrics', 'time');

-- –ò–Ω–¥–µ–∫—Å—ã –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
CREATE INDEX ON connections (hostname, time DESC);
CREATE INDEX ON connections (destination_address, time DESC);
CREATE INDEX ON connections (protocol, time DESC);
CREATE INDEX ON connections (process_name, time DESC);

CREATE INDEX ON protocol_stats (hostname, protocol, time DESC);
CREATE INDEX ON top_destinations (hostname, destination_address, time DESC);
CREATE INDEX ON system_metrics (hostname, time DESC);

-- –ü–æ–ª–∏—Ç–∏–∫–∏ —Ä–µ—Ç–µ–Ω—Ü–∏–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
SELECT add_retention_policy('connections', INTERVAL '30 days');
SELECT add_retention_policy('protocol_stats', INTERVAL '90 days');
SELECT add_retention_policy('top_destinations', INTERVAL '90 days');
SELECT add_retention_policy('system_metrics', INTERVAL '1 year');
```

### YAML Processor

```python
#!/usr/bin/env python3
"""
YAML to PostgreSQL processor –¥–ª—è Grafana
–°–ª–µ–¥–∏—Ç –∑–∞ –ø–∞–ø–∫–æ–π —Å –æ—Ç—á–µ—Ç–∞–º–∏ –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –∏—Ö –≤ –ë–î
"""

import os
import yaml
import time
import hashlib
import psycopg2
import logging
from datetime import datetime
from typing import Dict, List, Any
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
import ipaddress


class YAMLProcessor:
    def __init__(self, postgres_config: Dict[str, str]):
        self.postgres_config = postgres_config
        self.processed_files = set()
        self.logger = self._setup_logger()
        
    def _setup_logger(self) -> logging.Logger:
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        return logging.getLogger('yaml_processor')
    
    def connect_postgres(self):
        """–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ PostgreSQL"""
        return psycopg2.connect(
            host=self.postgres_config['host'],
            database=self.postgres_config['database'],
            user=self.postgres_config['user'],
            password=self.postgres_config['password']
        )
    
    def process_yaml_file(self, file_path: str):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç YAML —Ñ–∞–π–ª –∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ PostgreSQL"""
        try:
            self.logger.info(f"Processing {file_path}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ñ–∞–π–ª –Ω–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–ª—Å—è —Ä–∞–Ω–µ–µ
            file_hash = self._get_file_hash(file_path)
            if file_hash in self.processed_files:
                return
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º YAML
            with open(file_path, 'r', encoding='utf-8') as f:
                data = yaml.safe_load(f)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ—Ä–º–∞—Ç (NetFlow –∏–ª–∏ legacy)
            if 'netflow_message' in data:
                self._process_netflow_data(data, file_path)
            else:
                self._process_legacy_data(data, file_path)
            
            self.processed_files.add(file_hash)
            self.logger.info(f"Successfully processed {file_path}")
            
        except Exception as e:
            self.logger.error(f"Error processing {file_path}: {e}")
    
    def _process_netflow_data(self, data: Dict, file_path: str):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç NetFlow —Ñ–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö"""
        netflow = data['netflow_message']
        system_info = data.get('system_information', {})
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –æ—Ç—á–µ—Ç–∞
        report_id = self._generate_report_id(file_path, data)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –±–∞–∑–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        hostname = system_info.get('hostname', 'unknown')
        os_info = system_info.get('os', {})
        os_name = os_info.get('name', 'unknown')
        os_version = os_info.get('version', 'unknown')
        
        # –í—Ä–µ–º—è —ç–∫—Å–ø–æ—Ä—Ç–∞
        export_time = datetime.fromtimestamp(
            netflow['header']['export_timestamp']
        )
        
        with self.connect_postgres() as conn:
            with conn.cursor() as cur:
                # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
                self._insert_connections(cur, netflow['flows'], hostname, 
                                       os_name, os_version, report_id, export_time)
                
                # 2. –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø—Ä–æ—Ç–æ–∫–æ–ª–æ–≤
                self._insert_protocol_stats(cur, data.get('flow_statistics', {}), 
                                          hostname, report_id, export_time)
                
                # 3. –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
                self._insert_system_metrics(cur, netflow, system_info, 
                                          hostname, report_id, export_time)
                
                # 4. –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–ø –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–π
                self._insert_top_destinations(cur, netflow['flows'], 
                                            hostname, report_id, export_time)
    
    def _insert_connections(self, cur, flows: List[Dict], hostname: str, 
                           os_name: str, os_version: str, report_id: str, 
                           timestamp: datetime):
        """–í—Å—Ç–∞–≤–ª—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è—Ö"""
        for flow in flows:
            cur.execute("""
                INSERT INTO connections (
                    time, hostname, source_address, destination_address,
                    source_port, destination_port, protocol, protocol_number,
                    packet_count, byte_count, direction, process_name,
                    tcp_flags, report_id, os_name, os_version
                ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            """, (
                timestamp,
                hostname,
                flow.get('source_address'),
                flow.get('destination_address'),
                flow.get('source_port'),
                flow.get('destination_port'),
                flow.get('protocol_name'),
                flow.get('protocol'),
                flow.get('packet_count', 0),
                flow.get('byte_count', 0),
                flow.get('meta', {}).get('direction', 'unknown'),
                flow.get('meta', {}).get('process', 'unknown'),
                flow.get('tcp_flags', 0),
                report_id,
                os_name,
                os_version
            ))
    
    def _insert_protocol_stats(self, cur, flow_stats: Dict, hostname: str, 
                              report_id: str, timestamp: datetime):
        """–í—Å—Ç–∞–≤–ª—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø—Ä–æ—Ç–æ–∫–æ–ª–æ–≤"""
        protocols = flow_stats.get('protocols', {})
        
        for protocol, count in protocols.items():
            cur.execute("""
                INSERT INTO protocol_stats (
                    time, hostname, protocol, connection_count,
                    total_bytes, total_packets, report_id
                ) VALUES (%s, %s, %s, %s, %s, %s, %s)
            """, (
                timestamp,
                hostname,
                protocol,
                count,
                flow_stats.get('total_bytes', 0),
                flow_stats.get('total_packets', 0),
                report_id
            ))
    
    def _insert_system_metrics(self, cur, netflow: Dict, system_info: Dict,
                              hostname: str, report_id: str, timestamp: datetime):
        """–í—Å—Ç–∞–≤–ª—è–µ—Ç —Å–∏—Å—Ç–µ–º–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏"""
        flows = netflow.get('flows', [])
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
        total_connections = len(flows)
        incoming = len([f for f in flows if f.get('meta', {}).get('direction') == 'incoming'])
        outgoing = len([f for f in flows if f.get('meta', {}).get('direction') == 'outgoing'])
        tcp_count = len([f for f in flows if f.get('protocol') == 6])
        udp_count = len([f for f in flows if f.get('protocol') == 17])
        icmp_count = len([f for f in flows if f.get('protocol') == 1])
        
        unique_processes = len(set(
            f.get('meta', {}).get('process', 'unknown') for f in flows
        ))
        unique_destinations = len(set(
            f.get('destination_address') for f in flows if f.get('destination_address')
        ))
        
        cur.execute("""
            INSERT INTO system_metrics (
                time, hostname, total_connections, incoming_connections,
                outgoing_connections, tcp_connections, udp_connections,
                icmp_connections, unique_processes, unique_destinations,
                os_name, os_version, report_id
            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
        """, (
            timestamp, hostname, total_connections, incoming, outgoing,
            tcp_count, udp_count, icmp_count, unique_processes,
            unique_destinations, 
            system_info.get('os', {}).get('name', 'unknown'),
            system_info.get('os', {}).get('version', 'unknown'),
            report_id
        ))
    
    def _insert_top_destinations(self, cur, flows: List[Dict], hostname: str,
                                report_id: str, timestamp: datetime):
        """–í—Å—Ç–∞–≤–ª—è–µ—Ç —Ç–æ–ø –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–π"""
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è–º
        destinations = {}
        for flow in flows:
            dest = flow.get('destination_address')
            port = flow.get('destination_port')
            if not dest:
                continue
                
            key = (dest, port)
            if key not in destinations:
                destinations[key] = {
                    'count': 0,
                    'bytes': 0,
                    'protocols': set(),
                    'processes': set()
                }
            
            destinations[key]['count'] += 1
            destinations[key]['bytes'] += flow.get('byte_count', 0)
            destinations[key]['protocols'].add(flow.get('protocol_name', 'unknown'))
            destinations[key]['processes'].add(
                flow.get('meta', {}).get('process', 'unknown')
            )
        
        # –¢–æ–ø 20 –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–π –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
        top_destinations = sorted(
            destinations.items(), 
            key=lambda x: x[1]['count'], 
            reverse=True
        )[:20]
        
        for (dest, port), stats in top_destinations:
            cur.execute("""
                INSERT INTO top_destinations (
                    time, hostname, destination_address, destination_port,
                    connection_count, total_bytes, protocols, processes, report_id
                ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
            """, (
                timestamp, hostname, dest, port, stats['count'],
                stats['bytes'], list(stats['protocols']), 
                list(stats['processes']), report_id
            ))
    
    def _get_file_hash(self, file_path: str) -> str:
        """–ü–æ–ª—É—á–∞–µ—Ç —Ö–µ—à —Ñ–∞–π–ª–∞ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        with open(file_path, 'rb') as f:
            return hashlib.md5(f.read()).hexdigest()
    
    def _generate_report_id(self, file_path: str, data: Dict) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –æ—Ç—á–µ—Ç–∞"""
        file_name = os.path.basename(file_path)
        timestamp = datetime.now().isoformat()
        return f"{file_name}_{timestamp}"


class YAMLWatcher(FileSystemEventHandler):
    """–°–ª–µ–¥–∏—Ç –∑–∞ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º–∏ –≤ –ø–∞–ø–∫–µ —Å –æ—Ç—á–µ—Ç–∞–º–∏"""
    
    def __init__(self, processor: YAMLProcessor):
        self.processor = processor
        
    def on_created(self, event):
        if event.is_dir:
            return
            
        if event.src_path.endswith('.yaml') or event.src_path.endswith('.yml'):
            # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∑–∞–ø–∏—Å–∏ —Ñ–∞–π–ª–∞
            time.sleep(1)
            self.processor.process_yaml_file(event.src_path)


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    postgres_config = {
        'host': os.getenv('POSTGRES_HOST', 'localhost'),
        'database': os.getenv('POSTGRES_DB', 'analyzer_metrics'),
        'user': os.getenv('POSTGRES_USER', 'analyzer_user'),
        'password': os.getenv('POSTGRES_PASSWORD', 'analyzer_password')
    }
    
    watch_directory = os.getenv('WATCH_DIRECTORY', './reports')
    
    processor = YAMLProcessor(postgres_config)
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ñ–∞–π–ª—ã
    if os.path.exists(watch_directory):
        for filename in os.listdir(watch_directory):
            if filename.endswith(('.yaml', '.yml')):
                file_path = os.path.join(watch_directory, filename)
                processor.process_yaml_file(file_path)
    
    # –ù–∞—á–∏–Ω–∞–µ–º –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ –∑–∞ –Ω–æ–≤—ã–º–∏ —Ñ–∞–π–ª–∞–º–∏
    event_handler = YAMLWatcher(processor)
    observer = Observer()
    observer.schedule(event_handler, watch_directory, recursive=False)
    observer.start()
    
    try:
        while True:
            time.sleep(10)
    except KeyboardInterrupt:
        observer.stop()
    observer.join()


if __name__ == '__main__':
    main()
```

## üìà –î–∞—à–±–æ—Ä–¥—ã Grafana

### –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö

```yaml
# grafana/provisioning/datasources/postgres.yml
apiVersion: 1

datasources:
  - name: GlacierDB
    type: postgres
    access: proxy
    url: postgres:5432
    database: analyzer_metrics
    user: analyzer_user
    secureJsonData:
      password: analyzer_password
    jsonData:
      sslmode: disable
      maxOpenConns: 25
      maxIdleConns: 25
      connMaxLifetime: 14400
      postgresVersion: 1400
      timescaledb: true
```

### –û—Å–Ω–æ–≤–Ω–æ–π –¥–∞—à–±–æ—Ä–¥

```json
{
  "dashboard": {
    "id": null,
    "title": "Network Glacier Dashboard",
    "tags": ["analyzer", "network", "security"],
    "timezone": "browser",
    "panels": [
      {
        "id": 1,
        "title": "Total Connections Over Time",
        "type": "timeseries",
        "targets": [
          {
            "rawSql": "SELECT time, SUM(total_connections) as connections FROM system_metrics WHERE $__timeFilter(time) GROUP BY time ORDER BY time",
            "format": "time_series"
          }
        ],
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 0}
      },
      {
        "id": 2,
        "title": "Protocol Distribution",
        "type": "piechart",
        "targets": [
          {
            "rawSql": "SELECT protocol as metric, SUM(connection_count) as value FROM protocol_stats WHERE $__timeFilter(time) GROUP BY protocol",
            "format": "table"
          }
        ],
        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 0}
      },
      {
        "id": 3,
        "title": "Top Destinations",
        "type": "table",
        "targets": [
          {
            "rawSql": "SELECT destination_address, destination_port, SUM(connection_count) as connections, SUM(total_bytes) as bytes FROM top_destinations WHERE $__timeFilter(time) GROUP BY destination_address, destination_port ORDER BY connections DESC LIMIT 10",
            "format": "table"
          }
        ],
        "gridPos": {"h": 8, "w": 24, "x": 0, "y": 8}
      },
      {
        "id": 4,
        "title": "Network Traffic Volume",
        "type": "timeseries",
        "targets": [
          {
            "rawSql": "SELECT time, SUM(byte_count) as bytes FROM connections WHERE $__timeFilter(time) GROUP BY time ORDER BY time",
            "format": "time_series"
          }
        ],
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 16}
      },
      {
        "id": 5,
        "title": "Connection Directions",
        "type": "stat",
        "targets": [
          {
            "rawSql": "SELECT direction, COUNT(*) as connections FROM connections WHERE $__timeFilter(time) GROUP BY direction",
            "format": "table"
          }
        ],
        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 16}
      }
    ],
    "time": {
      "from": "now-6h",
      "to": "now"
    },
    "refresh": "5s",
    "schemaVersion": 30
  }
}
```

## üöÄ –ó–∞–ø—É—Å–∫ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

### 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è

```bash
# –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
mkdir -p grafana/{provisioning/datasources,provisioning/dashboards,dashboards}
mkdir -p reports
mkdir -p yaml-processor
mkdir -p sql

# –°–æ–∑–¥–∞–µ–º Dockerfile –¥–ª—è YAML –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
cat > yaml-processor/Dockerfile << 'EOF'
FROM python:3.9-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install -r requirements.txt

COPY yaml_processor.py .

CMD ["python", "yaml_processor.py"]
EOF

# –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –¥–ª—è YAML –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
cat > yaml-processor/requirements.txt << 'EOF'
pyyaml>=6.0
psycopg2-binary>=2.9.0
watchdog>=2.1.0
EOF
```

### 2. –ó–∞–ø—É—Å–∫ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã

```bash
# –ó–∞–ø—É—Å–∫–∞–µ–º –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã
docker-compose -f docker-compose.grafana.yml up -d

# –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å
docker-compose -f docker-compose.grafana.yml ps
```

### 3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ Glacier

```bash
# –ó–∞–ø—É—Å–∫–∞–µ–º Glacier (–æ—Ç—á–µ—Ç –ø–æ–ø–∞–¥–µ—Ç –≤ ./reports/)
cd glacier
python3 src/glacier.py --times 1 --output-dir ../reports/
```

### 4. –î–æ—Å—Ç—É–ø –∫ Grafana

```
URL: http://localhost:3000
–õ–æ–≥–∏–Ω: admin
–ü–∞—Ä–æ–ª—å: analyzer_admin
```

## üìä –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–∞—à–±–æ—Ä–¥–æ–≤

### 1. **Overview Dashboard**
- –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
- –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –ø—Ä–æ—Ç–æ–∫–æ–ª–∞–º
- –¢—Ä–µ–Ω–¥—ã –≤–æ –≤—Ä–µ–º–µ–Ω–∏
- –¢–æ–ø –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–π

### 2. **Security Dashboard**
- –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
- –ù–æ–≤—ã–µ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è
- –ê–Ω–æ–º–∞–ª—å–Ω—ã–π —Ç—Ä–∞—Ñ–∏–∫
- –ê–ª–µ—Ä—Ç—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏

### 3. **Performance Dashboard**
- Bandwidth utilization
- Connection patterns
- Process analytics
- Geographic distribution

### 4. **Historical Dashboard**
- –î–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ —Ç—Ä–µ–Ω–¥—ã
- –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–µ—Ä–∏–æ–¥–æ–≤
- Capacity planning
- Growth patterns

## üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è

### Retention Policies
```sql
-- –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–æ–ª–∏—Ç–∏–∫ —Ö—Ä–∞–Ω–µ–Ω–∏—è
SELECT add_retention_policy('connections', INTERVAL '30 days');
SELECT add_retention_policy('protocol_stats', INTERVAL '90 days');
```

### –ê–ª–µ—Ä—Ç—ã
```yaml
# –ü—Ä–∏–º–µ—Ä –∞–ª–µ—Ä—Ç–∞ –Ω–∞ –∞–Ω–æ–º–∞–ª—å–Ω—É—é –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
- alert: HighConnectionCount
  expr: max(system_metrics.total_connections) > 1000
  for: 5m
  annotations:
    summary: "High connection count detected"
```

## üìù –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

–≠—Ç–∞ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç:
- ‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é –∑–∞–≥—Ä—É–∑–∫—É –æ—Ç—á–µ—Ç–æ–≤
- ‚úÖ Real-time –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é
- ‚úÖ –ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ
- ‚úÖ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–µ –¥–∞—à–±–æ—Ä–¥—ã
- ‚úÖ –ê–ª–µ—Ä—Ç—ã –∏ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è
- ‚úÖ –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å 